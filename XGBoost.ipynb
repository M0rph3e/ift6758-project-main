{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "XGBoost.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM/pmDyRNjQl733PpsdBkRs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/M0rph3e/ift6758-project-main/blob/fengineering2/XGBoost.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBGc5ERyZQ9_"
      },
      "source": [
        "#Motivation for the code and explanation [here](https://towardsdatascience.com/binary-classification-xgboost-hyperparameter-tuning-scenarios-by-non-exhaustive-grid-search-and-c261f4ce098d)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvSwjChccUbn"
      },
      "source": [
        "#import comet_ml in the top of your file\n",
        "from comet_ml import Experiment\n",
        "import os\n",
        "from copy import deepcopy\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from skopt import BayesSearchCV\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
        "import xgboost as xgb\n",
        "random_state = 42"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "-fDgkwzTh1iH",
        "outputId": "a847d57f-2705-4f7c-e5fa-24e616da30cd"
      },
      "source": [
        "COMET_API_KEY = \n",
        "\n",
        "#create an experiment with your api key\n",
        "exp = Experiment(api_key=os.environ.get(COMET_API_KEY),\n",
        "                 project_name='XGBoost',\n",
        "                 auto_param_logging=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-e358681bff1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#create an experiment with your api key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m exp = Experiment(project_name='sklearn-demos',\n\u001b[0;32m----> 3\u001b[0;31m                  auto_param_logging=False)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/comet_ml/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, api_key, project_name, workspace, log_code, log_graph, auto_param_logging, auto_metric_logging, parse_args, auto_output_logging, log_env_details, log_git_metadata, log_git_patch, disabled, log_env_gpu, log_env_host, display_summary, log_env_cpu, display_summary_level, optimizer_data, auto_weight_logging, auto_log_co2, auto_metric_step_rate, auto_histogram_tensorboard_logging, auto_histogram_epoch_rate, auto_histogram_weight_logging, auto_histogram_gradient_logging, auto_histogram_activation_logging)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m             raise ValueError(\n\u001b[0;32m--> 262\u001b[0;31m                 \u001b[0;34m\"Comet.ml requires an API key. Please provide as the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m                 \u001b[0;34m\"first argument to Experiment(api_key) or as an environment\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m                 \u001b[0;34m\" variable named COMET_API_KEY \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Comet.ml requires an API key. Please provide as the first argument to Experiment(api_key) or as an environment variable named COMET_API_KEY "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQphAKJWoE0V"
      },
      "source": [
        "### Script to get the DataFrame and Split the data into train and test set for FE1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjFbDBdqiC28"
      },
      "source": [
        "# Apply XGB on distance and angle features from FE1\n",
        "xgb1 = xgb.XGBClassifier(objective='binary:logistic',\n",
        "                          booster='gbtree',\n",
        "                          eval_metric='auc',\n",
        "                          tree_method='gpu_hist',\n",
        "                          grow_policy='lossguide',\n",
        "                          use_label_encoder=False,\n",
        "                         random_state=random_state\n",
        "                        )\n",
        "xgb1.fit(X, y)\n",
        "\n",
        "y_pred = xgb1.predict(X_test)\n",
        "y_proba = xgb1.predict_proba(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDKOtCcnoDOK"
      },
      "source": [
        "###Script to perfrom generation of Graphs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbtA26whq5PJ"
      },
      "source": [
        "### Script to get the DataFrame and Split the data into train and test set for FE2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjqxy8F6rAFz"
      },
      "source": [
        "### Apply XGB on features from FE2\n",
        "#dictionary for collecting results\n",
        "results_dict = {}\n",
        "\n",
        "#obtaining default parameters by calling .fit() to XGBoost model instance\n",
        "xgb2 = xgb.XGBClassifier(objective='binary:logistic',\n",
        "                          booster='gbtree',\n",
        "                          eval_metric='auc',\n",
        "                          tree_method='gpu_hist',\n",
        "                          grow_policy='lossguide',\n",
        "                          use_label_encoder=False,\n",
        "                         random_state=random_state\n",
        "                        )\n",
        "xgb2.fit(X, y)\n",
        "\n",
        "#extracting default parameters from benchmark model\n",
        "default_params = {}\n",
        "gparams = xgb2.get_params()\n",
        "\n",
        "#default parameters have to be wrapped in lists - even single values - so GridSearchCV can take them as inputs\n",
        "for key in gparams.keys():\n",
        "    gp = gparams[key]\n",
        "    default_params[key] = [gp]\n",
        "\n",
        "#benchmark model. Grid search is not performed, since only single values are provided as parameter grid.\n",
        "#However, cross-validation is still executed\n",
        "clf0 = GridSearchCV(estimator=xgb2, scoring='accuracy', param_grid=default_params, return_train_score=True, verbose=1, cv=3)\n",
        "clf0.fit(X_train, y_train.values.ravel())\n",
        "\n",
        "#results dataframe\n",
        "df = pd.DataFrame(clf0.cv_results_)\n",
        "\n",
        "#best parameters\n",
        "bp = clf0.best_params_\n",
        "\n",
        "#storing computed values in results dictionary\n",
        "results_dict['xgb2'] = {'iterable_parameter': np.nan,\n",
        "                         'classifier': deepcopy(clf0),\n",
        "                         'cv_results': df.copy(),\n",
        "                         'best_params': bp}\n",
        "\n",
        "\n",
        "\n",
        "y_pred = clf0.predict(X_test)\n",
        "y_proba = clf0.predict_proba(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBMiqR0qrI6z"
      },
      "source": [
        "##Hyperparameter Tuning\n",
        "#creating deepcopy of default parameters before manipulations\n",
        "params = deepcopy(default_params)\n",
        "\n",
        "#setting grid of selected parameters for iteration\n",
        "param_grid = {'gamma': [0,0.1,0.2,0.4,0.8,1.6,3.2,6.4,12.8,25.6,51.2,102.4, 200],\n",
        "              'learning_rate': [0.01, 0.03, 0.06, 0.1, 0.15, 0.2, 0.25, 0.3, 0.4, 0.5, 0.6, 0.7],\n",
        "              'max_depth': [5,6,7,8,9,10,11,12,13,14],\n",
        "              'n_estimators': [50,65,80,100,115,130,150],\n",
        "              'reg_alpha': [0,0.1,0.2,0.4,0.8,1.6,3.2,6.4,12.8,25.6,51.2,102.4,200],\n",
        "              'reg_lambda': [0,0.1,0.2,0.4,0.8,1.6,3.2,6.4,12.8,25.6,51.2,102.4,200]}\n",
        "\n",
        "#No. of jobs\n",
        "bcvj = int(np.cumsum([len(x) for x in param_grid.values()])[-1])\n",
        "\n",
        "#unwrapping list values of default parameters\n",
        "default_params_xgb = {}\n",
        "\n",
        "for key in default_params.keys():\n",
        "    default_params_xgb[key] = default_params[key][0]\n",
        "\n",
        "#providing default parameters to xgbc model, before randomized search cross-validation\n",
        "xgb3 = xgb.XGBClassifier(**default_params_xgb)\n",
        "\n",
        "clf = BayesSearchCV(estimator=xgb3, search_spaces=param_grid, n_iter=bcvj, scoring='accuracy', cv=3, return_train_score=True, verbose=3)\n",
        "clf.fit(X_train, y_train.values.ravel())\n",
        "\n",
        "#results dataframe\n",
        "df = pd.DataFrame(clf.cv_results_)\n",
        "\n",
        "#best parameters\n",
        "bp = clf.best_params_\n",
        "\n",
        "#storing computed values in results dictionary\n",
        "results_dict['xgb3'] = {'classifier': deepcopy(clf),\n",
        "                            'cv_results': df.copy(),\n",
        "                            'best_params': bp}\n",
        "\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "y_proba = clf.predict_proba(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nw_ZO7O3rBoT"
      },
      "source": [
        "###Script to perfrom generation of Graphs"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}