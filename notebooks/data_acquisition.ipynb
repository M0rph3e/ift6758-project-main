{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7723d939",
   "metadata": {},
   "source": [
    "# Notebook for data acquisition functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73d568d",
   "metadata": {},
   "source": [
    "#### The first fctn take he target year and a filepath as argument to download the data from a specific season and put it in a certain path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2143eeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac4c8b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6a06237",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_season_data(year, file_path):\n",
    "    \"\"\"\n",
    "    Get the data from the Hockey API in Pickle format for a specific season (year-year+1) and store them to a given file_path. Pickle contains list[dicts of all games in the season]\n",
    "\n",
    "    :param year: year of the season (example 2017 will download Season 2017-2018)\n",
    "    :type year: int\n",
    "    :param file_path: Directory where the pickle file for entire season is stored are going to be saved\n",
    "    :type file_path: str\n",
    "\n",
    "    :rtype: list, list[dict]\n",
    "    :return: list[ dicts of all games in the season],\n",
    "\n",
    "    \"\"\"\n",
    "    YEAR = year\n",
    "    DIRECTORY  = f\"{file_path}/PICKLE/\"\n",
    "    PATH = f\"{DIRECTORY}/{YEAR}.pkl\"\n",
    "    MAX_GAMES=1300\n",
    "    os.makedirs(DIRECTORY, exist_ok=True)\n",
    "    season_types = [\"01\", \"02\", \"03\", \"04\"]\n",
    "    games_list = []\n",
    "    if os.path.isfile(PATH):\n",
    "        with open(PATH, 'rb') as f:\n",
    "            games_list = pickle.load(f)\n",
    "\n",
    "    else:\n",
    "        for season_type in season_types:\n",
    "            for g in tqdm(range(1,MAX_GAMES)):\n",
    "                game_number = str(g).zfill(4)\n",
    "                GAME_ID = f\"{YEAR}{season_type}{game_number}\"\n",
    "                r = requests.get(f\"https://statsapi.web.nhl.com/api/v1/game/{GAME_ID}/feed/live/\")\n",
    "                if r.status_code == 200:\n",
    "                    ## Storing as dicts\n",
    "                    game_dict = r.json()\n",
    "                    games_list.append(game_dict)\n",
    "        with open(PATH,'wb') as f:\n",
    "            pickle.dump(games_list,f)\n",
    "    # print(f\"Len of games_list in {year} is {len(games_list)}\")\n",
    "\n",
    "    return games_list\n",
    "                    \n",
    "                    \n",
    "                    # with open(PATH, 'wb') as f:\n",
    "                    #     f.write(r.content)  \n",
    "            #else:\n",
    "               # print(\"File already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b8a3687",
   "metadata": {},
   "outputs": [],
   "source": [
    "season2016=get_season_data(2016, \"../ift6758/data/\")\n",
    "season2017=get_season_data(2017, \"../ift6758/data/\")\n",
    "season2018=get_season_data(2018, \"../ift6758/data/\")\n",
    "season2019=get_season_data(2019, \"../ift6758/data/\")\n",
    "season2020=get_season_data(2020, \"../ift6758/data/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1fa895",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f1950db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already Exists, loading from ../ift6758/data//PICKLE//2016.pkl\n",
      "Len of games_list in 2016 is 1441\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from ift6758.data.data_acquisition import Season\n",
    "Season2016=Season(2016,\"../ift6758/data/\")\n",
    "season2016_data = Season2016.get_season_data()\n",
    "print(season2016 == season2016_data) #Verifying if we are loading same data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "092a2211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a=Season2016+Season2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6f9e86d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2882"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27173861",
   "metadata": {},
   "source": [
    "## From the api documentation the max of matches per season is less than 1300. For security, we put 1300 to check all the ids where we can find a match on the data. We could have put a break after we have a 404 answer from the request (for efficency). But we assumed that the data might miss some game ids. And we download them just once. So no need for time efficiency for now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be429e2f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fe0e4489f2919cda2ef5940e6c22f887a5b71570af9d1a74ba05de65e7f388ec"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
